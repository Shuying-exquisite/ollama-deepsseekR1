# ollama-deepsseekR1
部署在服务器的deepseekR1模型
1.打开豆包登陆账号创建项目https://www.marscode.cn/
2.在终端输入ollama回车运行，提示下载ollama回车
3.在终端输入ollama serve（启动ollama）
4.点击拆分终端，在另一个终端中输入ollama -v（查看ollama是否运行，正常运行会显示ollama的版本）
5.在刚刚查询ollama运行的终端中输入ollama run deepseek-r1:1.5b下载并运行deepseek模型
6.保持deepsek模型的运行
7.下载插件continue，添加ollama deepseek模型自动检测
